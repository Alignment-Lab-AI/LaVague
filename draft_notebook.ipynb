{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "world_model_examples = \"\"\"\n",
    "Objective:  What's the name of our Lead Developer ?\n",
    "Previous instructions: [NONE]\n",
    "Last engine: [NONE]\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: The organization is comprised of several teams, each with a distinct focus. The Software Development Team creates custom applications, while the IT Support Team ensures the technology infrastructure runs smoothly. The Quality Assurance Team rigorously tests products to maintain high standards of quality. The Research and Development Team explores new technologies and methodologies, and the Systems Engineering Team designs and maintains complex systems that power the organization's services.\n",
    "\n",
    "Thoughts:\n",
    "- The screenshot shows a Notion page.\n",
    "- The page content summary provides an overview of the organization's structure and the roles of different teams.\n",
    "- The \"Our Departments\" section lists various departments, including \"Engineering Department.\"\n",
    "- To find the name of the Lead Developer, I should look for information under the \"Engineering Department\" section.\n",
    "- The \"Engineering Department\" section is likely to contain details about the team, including the Lead Developer.\n",
    "Next engine: Navigation Engine\n",
    "Instruction: Locate the link titled \"Engineering Department\" and click on it.\n",
    "-----\n",
    "Objective:  Go to the first issue you can find\n",
    "Previous instructions: \n",
    "- Click on 'Issues' with the number '28' next to it.\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: The provided text appears to be a list of issues or tasks related to the LaVague project. The issues are categorized into different types, including bugs, enhancements, and documentation improvements. The issues range from debugging guides to documentation improvements, and even feature requests. Some issues are labeled as \"help wanted,\" indicating that they require additional attention or expertise. The text also includes information about the status of the issues, such as whether they are open, in progress, or have been closed\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the issues page of the repository 'lavague-ai/LaVague'.\n",
    "- The summary indicates that the page contains a list of issues related to the LaVague project.\n",
    "- The objective is to go to the first issue.\n",
    "- The first issue is highlighted in the list of issues.\n",
    "Next engine: Navigation Engine\n",
    "Instruction: Click on the issue, with title 'Build and share place where people can suggest their use cases and results #225'\n",
    "-----\n",
    "Objective: Find When Llama 3 was released\n",
    "Previous instructions:\n",
    "- Click on 'meta-llama/Meta-Llama-3-8B'\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: The provided text discusses the development and release of Llama 3, a large language model designed for various applications. The model's development prioritizes responsible AI development, limiting misuse and harm, and supporting the open-source community. \n",
    "The model's safety features include safeguards such as Meta Llama Guard 2 and Code Shield, which drastically reduce residual risks while maintaining helpfulness. The model's safety is ensured through extensive testing, adversarial evaluations, and implemented safety mitigations. \n",
    "The text also highlights the importance of responsible use, emphasizing the need for developers to implement safety best practices and tailor safeguards to their specific use case and audience. The model's refusals to benign prompts have been improved, reducing the likelihood of falsely refusing to answer prompts. \n",
    "The release of Llama 3 followed a rigorous process, including extra measures against misuse and critical risks. The model's safety has been assessed in various areas, including CBRNE threats, cyber security, and child safety. The company is actively contributing to open consortiums, promoting safety standardization and transparency in the development of generative AI.\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the model page for 'meta-llama/Meta-Llama-3-8B' on Hugging Face.\n",
    "- The page content summary provides detailed information about the development and release of Llama 3, emphasizing its safety features and responsible AI development.\n",
    "- Hugging Face, is a hub for AI models and datasets, where users can explore and interact with a variety of AI models.\n",
    "- I am therefore on the right page to find information about the release date of 'Meta-Llama-3-8B'.\n",
    "- To find the release date, I need to locate the relevant information in the content of the page.\n",
    "- Therefore the best next step is to use the Python Engine to extract the release date from the content of the page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the release date of 'Meta-Llama-3-8B' from the textual content of the page.\n",
    "-----\n",
    "Objective: Provide the code to get started with Gemini API\n",
    "Previous instructions:\n",
    "- Click on 'Read API docs'\n",
    "- Click on 'Gemini API quickstart' on the menu\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: The text provides an introduction to the Gemini API, a family of Google's most capable AI models. It offers a comprehensive guide to help users get started with the API. The fastest way to start using Gemini is through Google AI Studio, a web-based tool that allows users to prototype, run prompts, and get started with the API. The text also provides a quickstart guide to help users begin. Additionally, it emphasizes the importance of using LLMs safely and responsibly, directing users to safety settings and safety guidance documentation. The text also mentions that the Gemini API and Google AI Studio are available in over 180 countries. Finally, it provides further reading resources for users to learn more about the models provided by the Gemini API.\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the documentation page for the getting started of Gemini API.\n",
    "- The page content summary provides an overview of the Gemini API, highlighting its capabilities and the code to get started.\n",
    "- I am therefore on the right page to find the code to get started with the Gemini API.\n",
    "- The next step is to provide the code to get started with the Gemini API.\n",
    "- Therefore I need to use the Python Engine to generate the code to extract the code to get started with the Gemini API from this page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the code to get started with the Gemini API from the content of the page.\n",
    "-----\n",
    "Objective: Show what is the cheapest product\n",
    "Previous instructions: [NONE]\n",
    "Last engine: [NONE]\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: The text provides an overview of the products available on the website. The products are categorized into different sections, including electronics, clothing, accessories, and home goods. Each product listing includes details such as the product name, price, and description. \n",
    "\n",
    "Thoughts:\n",
    "- The screenshot shows an e-commerce website with various products.\n",
    "- The page content summary describes the different product categories available on the website.\n",
    "- To find the cheapest product, I need to identify the product with the lowest price.\n",
    "- There seems to be selectors for sorting products by price on the left side of the page.\n",
    "- The screenshot only shows part of the selectors for price. I should scroll down to see the full list of products and prices.\n",
    "Next engine: Navigation Controls\n",
    "Instruction: SCROLL_DOWN\n",
    "\"\"\"\n",
    "\n",
    "WORLD_MODEL_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in high level reasoning. Your goal is to generate instructions for other specialized AIs to perform web actions to reach objectives given by humans.\n",
    "Your inputs are:\n",
    "- objective ('str'): a high level description of the goal to achieve.\n",
    "- previous_instructions ('str'): a list of previous steps taken to reach the objective.\n",
    "- last_engine ('str'): the engine used in the previous step.\n",
    "- current_relevant_state ('obj'): the state of the environment to use to perform the next step. This can be a screenshot if the previous engine was a NavigationEngine, or description of variables if the previous engine was a PythonEngine.\n",
    "\n",
    "Your output are:\n",
    "- thoughts ('str'): a list of thoughts in bullet points detailling your reasoning.\n",
    "- next_engine ('str'): the engine to use for the next step.\n",
    "- instruction ('str'): the instruction for the engine to perform the next step.\n",
    "\n",
    "Here are the engines at your disposal:\n",
    "- Python Engine: This engine is used when the task requires doing computing using the current state of the agent. \n",
    "It does not impact the outside world and does not navigate.\n",
    "- Navigation Engine: This engine is used when the next step of the task requires further navigation to reach the goal. \n",
    "For instance it can be used to click on a link or to fill a form on a webpage. This engine is heavy and will do complex processing of the current HTML to decide which element to interact with.\n",
    "- Navigation Controls: This is a simpler engine to do commands that does not require reasoning, which are 'SCROLL_DOWN', 'SCROLL_UP' and 'WAIT'.\n",
    "\n",
    "The instruction should be detailled as possible and only contain the next step. \n",
    "When providing an instruction to the Python Engine, do not provide any guideline on using visual information such as the screenshot, as the Python Engine does not have access to it.\n",
    "If the screenshot provides information but seems insufficient, use navigation controls to further explore the page.\n",
    "If the objective is already achieved in the screenshot, provide the next engine and instruction 'STOP'.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next objective:\n",
    "Objective: {objective}\n",
    "Previous instructions: \n",
    "{previous_instructions}\n",
    "Last engine: {last_engine}\n",
    "Current relevant state: {current_state}\n",
    "\n",
    "Thought:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- IO Engine: This engine is used when the task requires interacting with the outside world, such at the end of an agent run, to send the result of the agent to the user.\\nFor instance, it can be used to send an email with the result of the agent, or pull data from a database to update the agent state.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "- IO Engine: This engine is used when the task requires interacting with the outside world, such at the end of an agent run, to send the result of the agent to the user.\n",
    "For instance, it can be used to send an email with the result of the agent, or pull data from a database to update the agent state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:18: UserWarning: \u001b[93mTelemetry is turned on. To turn off telemetry, set your TELEMETRY_VAR to 'NONE'\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n",
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:24: UserWarning: \u001b[93mSecurity warning: This package executes LLM-generated code. Consider using this package in a sandboxed environment.\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from lavague.contexts.openai import OpenaiContext\n",
    "openai_context = OpenaiContext(llm=\"gpt-4o\")\n",
    "\n",
    "from lavague.contexts.gemini import GeminiContext\n",
    "context = GeminiContext()\n",
    "\n",
    "context.mm_llm = openai_context.mm_llm\n",
    "\n",
    "mm_llm = context.mm_llm\n",
    "prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "    examples=world_model_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional\n",
    "from lavague.core import Context, get_default_context\n",
    "from llama_index.core.base.llms.base import BaseLLM\n",
    "import copy\n",
    "\n",
    "PYTHON_ENGINE_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in Python code generation to answer user queries.\n",
    "The inputs are: an instruction, and the current state of the local variables available to the environment where your code will be executed.\n",
    "Your output is the code that will perform the action described in the instruction, using the variables available in the environment.\n",
    "You can import libraries and use any variables available in the environment.\n",
    "Detail thoroughly the steps to perform the action in the code you generate with comments.\n",
    "The last line of your code should be an assignment to the variable 'output' containing the result of the action.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Instruction: {instruction}\n",
    "State:\n",
    "{state_description}\n",
    "Code:\n",
    "\"\"\")\n",
    "\n",
    "class PythonEngine:\n",
    "    llm: BaseLLM    \n",
    "    prompt_template: PromptTemplate\n",
    "\n",
    "    def __init__(self, examples: str, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.extractor = context.extractor\n",
    "        self.prompt_template = PYTHON_ENGINE_PROMPT_TEMPLATE.partial_format(\n",
    "            examples=examples\n",
    "        )\n",
    "        \n",
    "    def generate_code(self, instruction: str, state: dict) -> str:\n",
    "        state_description = self.get_state_description(state)\n",
    "        prompt = self.prompt_template.format(instruction=instruction, state_description=state_description)\n",
    "        response = self.llm.complete(prompt).text\n",
    "        code = self.extractor.extract(response)\n",
    "        return code\n",
    "    \n",
    "    def execute_code(self, code: str, state: dict):\n",
    "        local_scope = copy.deepcopy(state)\n",
    "        exec(code, local_scope, local_scope)\n",
    "        output = local_scope[\"output\"]\n",
    "        return output\n",
    "        \n",
    "    def get_state_description(self, state: dict) -> str:\n",
    "        \"\"\"TO DO: provide more complex state descriptions\"\"\"\n",
    "        state_description = \"\"\"\n",
    "    html ('str'): The content of the HTML page being analyzed\"\"\"\n",
    "        return state_description\n",
    "    \n",
    "with open(\"python_engine_examples.txt\") as f:\n",
    "    python_engine_examples = f.read()\n",
    "    \n",
    "python_engine = PythonEngine(python_engine_examples, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITER_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI expert.\n",
    "You are given a high level instruction on a generic action to perform.\n",
    "Your output is an instruction of the action, rewritten to be more specific on the capabilities at your disposal to perform the action.\n",
    "Here are your capabilities:\n",
    "{capabilities}\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next instruction to rewrite:\n",
    "Original instruction: {original_instruction}\n",
    "\"\"\")\n",
    "\n",
    "DEFAULT_CAPABILITIES = \"\"\"\n",
    "- Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_EXAMPLES = \"\"\"\n",
    "Original instruction: Use the content of the HTML page to answer the question 'How was falcon-11B trained?'\n",
    "Capability: Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "Rewritten instruction: Extract the content of the HTML page and use llama index to answer the question 'How was falcon-11B trained?'\n",
    "\"\"\"\n",
    "\n",
    "class Rewriter:\n",
    "    def __init__(self, capabilities: str = DEFAULT_CAPABILITIES, examples: str = DEFAULT_EXAMPLES, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.prompt_template = REWRITER_PROMPT_TEMPLATE.partial_format(\n",
    "            capabilities=capabilities,\n",
    "            examples=examples\n",
    "        )      \n",
    "    def rewrite_instruction(self, original_instruction: str) -> str:\n",
    "        prompt = self.prompt_template.format(original_instruction=original_instruction)\n",
    "        rewritten_instruction = self.llm.complete(prompt).text\n",
    "        return rewritten_instruction\n",
    "    \n",
    "rewriter = Rewriter(capabilities=DEFAULT_CAPABILITIES, examples=DEFAULT_EXAMPLES, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flemme de finir\n",
    "\n",
    "# Thoughts:\n",
    "# - The current page is the Azure Pricing Calculator.\n",
    "# - To calculate the cost of an AKS (Azure Kubernetes Service) cluster, we need to add the AKS service to the calculator.\n",
    "# - The current screen does not show the options to add services.\n",
    "# - The best approach is to scroll down to find the section where services can be added to the calculator.\n",
    "# - Because this is a simple scrolling action, the best next step is to use the Navigation Controls engine to scroll down.\n",
    "# Next engine: Navigation Controls\n",
    "# Instruction: Scroll down by one full screen to continue exploring the current page.\n",
    "# -----\n",
    "# Objective: Find the definition of 'Diffusers'\n",
    "# Previous instructions: \n",
    "# - Click on 'Diffusers' link\n",
    "# Last engine: Navigation Engine\n",
    "# Current state: [SCREENSHOT]\n",
    "\n",
    "# Thought:\n",
    "# - The current page is the documentation page of Hugging Face.\n",
    "# - Hugging Face is a platform for AI models and datasets, where users can explore and interact with latest AI resources.\n",
    "# - The definition of 'Diffusers' is provided in the documentation.\n",
    "# - No further action is needed to achieve the objective.\n",
    "# Next engine: STOP\n",
    "# Instruction: STOP -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "width = 1024\n",
    "height = 1024\n",
    "\n",
    "driver.set_window_size(width, height)\n",
    "viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "\n",
    "height_difference = height - viewport_height\n",
    "driver.set_window_size(width, height + height_difference)\n",
    "\n",
    "driver.get(\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "import trafilatura\n",
    "\n",
    "class PageSummarizer:\n",
    "    def __init__(self, llm, embed_model):\n",
    "        self.llm = llm\n",
    "        self.embed_model = embed_model\n",
    "        \n",
    "    def summarize(self, html: str) -> str:\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "        \n",
    "        page_content = trafilatura.extract(html)\n",
    "        \n",
    "        documents = [Document(text=page_content)]\n",
    "        index = VectorStoreIndex.from_documents(documents)\n",
    "        query_engine = index.as_query_engine()\n",
    "        instruction = \"Provide a detailled summary of this text\"\n",
    "        page_content_summary = query_engine.query(instruction).response\n",
    "        return page_content_summary\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "model = \"llama3-8b-8192\"\n",
    "llm = Groq(model=model, temperature=0.1)\n",
    "embed_model = context.embedding\n",
    "\n",
    "page_summarizer = PageSummarizer(llm, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_instruction(text):\n",
    "    # Use a regular expression to find the content after \"Instruction:\"\n",
    "    instruction_patterns = [\n",
    "        r\"Instruction:\\s*(.*)\",\n",
    "        r\"### Instruction:\\s*(.*)\"\n",
    "    ]\n",
    "    for pattern in instruction_patterns:\n",
    "        instruction_match = re.search(pattern, text)\n",
    "        if instruction_match:\n",
    "            return instruction_match.group(\n",
    "                1\n",
    "            ).strip()  # Return the matched group, stripping any excess whitespace\n",
    "        \n",
    "    raise ValueError(\"No instruction found in the text.\")\n",
    "\n",
    "def extract_next_engine(text):\n",
    "    # Use a regular expression to find the content after \"Next engine:\"\n",
    "    \n",
    "    next_engine_patterns = [\n",
    "        r\"Next engine:\\s*(.*)\",\n",
    "        r\"### Next Engine:\\s*(.*)\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in next_engine_patterns:\n",
    "        next_engine_match = re.search(pattern, text)\n",
    "        if next_engine_match:\n",
    "            return next_engine_match.group(\n",
    "                1\n",
    "            ).strip()\n",
    "    raise ValueError(\"No next engine found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import ActionEngine\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "\n",
    "selenium_driver = SeleniumDriver()\n",
    "selenium_driver.driver = driver\n",
    "navigation_engine = ActionEngine(selenium_driver)\n",
    "action_engine = navigation_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# TO DO: Generalize to BaseDriver to support other drivers\n",
    "\n",
    "class NavigationControl:\n",
    "    def __init__(self, driver) -> None:\n",
    "        self.driver = driver\n",
    "        \n",
    "    def execute_instruction(self, instruction):\n",
    "        if instruction == 'SCROLL_DOWN':\n",
    "            driver = self.driver\n",
    "            body = driver.find_element(By.TAG_NAME, 'body')\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "        elif instruction == 'SCROLL_UP':\n",
    "            driver = self.driver\n",
    "            body = driver.find_element(By.TAG_NAME, 'body')\n",
    "            body.send_keys(Keys.PAGE_UP)\n",
    "        elif instruction == 'WAIT':\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown instruction: {instruction}\")\n",
    "        \n",
    "\n",
    "navigation_control = NavigationControl(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughts:\n",
      "- The screenshot shows the documentation page of Hugging Face.\n",
      "- The page content summary provides an overview of various products and services offered by Hugging Face.\n",
      "- The objective is to provide the code to install PEFT.\n",
      "- The \"PEFT\" section is visible in the screenshot.\n",
      "- I need to navigate to the PEFT section to find the installation code.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the \"PEFT\" section to navigate to its documentation.\n",
      "Thoughts:\n",
      "- The current screenshot shows the documentation page for the PEFT library on Hugging Face.\n",
      "- The page content summary provides an overview of the PEFT library and its integration with other libraries.\n",
      "- To provide the code to install PEFT, I need to locate the installation instructions within the documentation.\n",
      "- The installation instructions are likely to be found in the \"Quickstart\" or \"Installation\" section of the documentation.\n",
      "- The \"Quickstart\" section is visible in the left-hand menu.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the \"Quickstart\" section in the left-hand menu to locate the installation instructions for PEFT.\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Thoughts:\n",
      "- The current screenshot shows the documentation page for PEFT on Hugging Face.\n",
      "- The page content summary provides an overview of PEFT and its integration with other libraries.\n",
      "- The objective is to provide the code to install PEFT.\n",
      "- The \"Quickstart\" section in the left-hand menu is likely to contain the installation instructions.\n",
      "- The \"Quickstart\" section is already highlighted in the left-hand menu, indicating that we are in the correct section.\n",
      "- I need to locate the specific installation code within the \"Quickstart\" section.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Extract the code to install PEFT from the content of the \"Quickstart\" section.\n",
      "Output generated by Python Engine:  There is no code snippet related to installing PEFT in the provided context information.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the \"Quickstart\" section from the HTML page. \n",
      "# Then we will use an LLM to identify and extract the code snippet related to installing PEFT. Because the page content might not fit in the LLM context window, we will use Llama Index to perform RAG on the extracted text content.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the \"Quickstart\" section from the HTML page\n",
      "import trafilatura\n",
      "\n",
      "page_content = trafilatura.extract(html, include_links=False, include_images=False, include_tables=False, include_comments=False, include_scripts=False, include_stylesheets=False, include_metadata=False, include_title=False, include_body=False, include_headings=False, include_paragraphs=False, include_lists=False, include_quotes=False, include_code=False, include_text=True, include_section=True, section_selector='h2:contains(\"Quickstart\")')\n",
      "\n",
      "# Next we will use Llama Index to perform RAG on the extracted text content\n",
      "from llama_index.core import Document, VectorStoreIndex\n",
      "\n",
      "documents = [Document(text=page_content)]\n",
      "\n",
      "# We then build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      "# We will use the query engine to answer the question\n",
      "instruction = \"Extract the code snippet related to installing PEFT\"\n",
      "\n",
      "# We finally store the output in the variable 'output'\n",
      "output = query_engine.query(instruction)\n",
      "- The current screenshot shows the PEFT documentation page on Hugging Face.\n",
      "- The \"Quickstart\" section in the left-hand menu is highlighted, indicating that we are in the correct section.\n",
      "- The previous Python Engine output indicates that there is no code snippet related to installing PEFT in the provided context information.\n",
      "- I need to further explore the page to locate the installation instructions for PEFT.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4490/3571914547.py\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         last_engine=last_engine, current_state=current_state)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmm_llm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm_llm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/llama_index/multi_modal_llms/openai/base.py\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, prompt, image_documents, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_documents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImageDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     ) -> CompletionResponse:\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     def stream_complete(\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/llama_index/multi_modal_llms/openai/base.py\u001b[0m in \u001b[0;36m_complete\u001b[0;34m(self, prompt, image_documents, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMessageRole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n\u001b[0;32m--> 220\u001b[0;31m         response = self._client.chat.completions.create(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    664\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     def patch(\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 889\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    919\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    930\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    226\u001b[0m         )\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionNotAvailable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;31m# The ConnectionNotAvailable exception is a special case, that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionNotAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mreason_phrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    112\u001b[0m                 trace.return_value = (\n\u001b[1;32m    113\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1290\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1292\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# url = \"https://maize-paddleboat-93e.notion.site/Welcome-to-ACME-INC-0ac66cd290e3453b93a993e1a3ed272f\"\n",
    "# objective = \"What's the name of our Head of Software?\"\n",
    "\n",
    "# url = \"https://huggingface.co/models\"\n",
    "# objective = \"Provide the code to use Falcon 11B\"\n",
    "\n",
    "url = \"https://huggingface.co/docs\"\n",
    "objective = \"Provide the code to install PEFT\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "previous_instructions = \"[NONE]\"\n",
    "last_engine = \"[NONE]\"\n",
    "\n",
    "N_ATTEMPTS = 5\n",
    "N_STEPS = 5\n",
    "\n",
    "driver.save_screenshot(\"screenshots/output.png\")\n",
    "image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "html = driver.page_source\n",
    "page_content_summary = page_summarizer.summarize(html)\n",
    "\n",
    "current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: {page_content_summary}\n",
    "\"\"\"\n",
    "\n",
    "for i in range(N_STEPS):\n",
    "    prompt = prompt_template.format(\n",
    "        objective=objective, previous_instructions=previous_instructions, \n",
    "        last_engine=last_engine, current_state=current_state)\n",
    "\n",
    "    mm_llm_output = mm_llm.complete(prompt, image_documents=image_documents).text\n",
    "\n",
    "    print(mm_llm_output)\n",
    "\n",
    "    next_engine = extract_next_engine(mm_llm_output)\n",
    "    instruction = extract_instruction(mm_llm_output)\n",
    "    \n",
    "    if next_engine == \"Navigation Engine\":\n",
    "        \n",
    "        query = instruction\n",
    "        nodes = action_engine.get_nodes(query)\n",
    "        llm_context = \"\\n\".join(nodes)\n",
    "\n",
    "        for _ in range(N_ATTEMPTS):\n",
    "            try:\n",
    "                action = action_engine.get_action_from_context(llm_context, query)\n",
    "                code = f\"\"\"\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "                {action}\"\"\".strip()\n",
    "\n",
    "                local_scope = {\"driver\": driver}\n",
    "                exec(code, local_scope, local_scope)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Action execution failed. Retrying...\")\n",
    "                pass\n",
    "            \n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "        html = driver.page_source\n",
    "        page_content_summary = page_summarizer.summarize(html)\n",
    "\n",
    "        current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "- page_content_summary: {page_content_summary}\n",
    "        \"\"\"\n",
    "            \n",
    "    elif next_engine == \"Python Engine\":\n",
    "        rewritten_instruction = rewriter.rewrite_instruction(instruction)\n",
    "        state = {\n",
    "            \"html\": driver.page_source\n",
    "        }\n",
    "\n",
    "        code = python_engine.generate_code(rewritten_instruction, state)\n",
    "        output = python_engine.execute_code(code, state)\n",
    "        print(\"Output generated by Python Engine: \", output)\n",
    "        print(\"Code generated by Python Engine: \", code)\n",
    "        \n",
    "        current_state = f\"\"\"\n",
    "- output: {output}\n",
    "\"\"\"\n",
    "    elif next_engine == \"Navigation Controls\":\n",
    "        navigation_control.execute_instruction(instruction)\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "        \n",
    "    # elif next_engine == \"IO Engine\":\n",
    "    #     print(\"IO Engine\")\n",
    "    #     break\n",
    "    elif next_engine == \"STOP\" or instruction == \"STOP\":\n",
    "        print(\"Objective reached. Stopping...\")\n",
    "        break\n",
    "    #     print(\"IO Engine\")\n",
    "    #     break\n",
    "    \n",
    "    if previous_instructions == \"[NONE]\":\n",
    "        previous_instructions = f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "    else:\n",
    "        previous_instructions += f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "        \n",
    "    last_engine = next_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeminiMultiModal(model_name='models/gemini-1.5-flash-latest', temperature=0.1, max_tokens=8192, generate_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigation Engine: <lavague.core.action_engine.ActionEngine object at 0x7f70c9a9ab00>\n",
      "Instruction: STOP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "### Next Engine:\n",
    "Navigation Engine\n",
    "\n",
    "### Instruction:\n",
    "Locate and click on the \"PEFT\" link to navigate to its documentation or installation guide.\n",
    "\"\"\"\n",
    "\n",
    "# Define the patterns to extract the required parts\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Navigation Engine:\", navigation_engine)\n",
    "print(\"Instruction:\", instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "page_content_summary = page_summarizer.summarize(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughts:\n",
      "- The Python Engine has already provided the answer to the objective.\n",
      "- The objective is already achieved.\n",
      "Next engine: STOP\n",
      "Instruction: STOP\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
