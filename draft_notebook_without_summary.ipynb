{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "world_model_examples = \"\"\"\n",
    "Objective:  Go to the first issue you can find\n",
    "Previous instructions: \n",
    "- Click on 'Issues' with the number '28' next to it.\n",
    "- [FAILED] Click on 'Build and share place where people can suggest their use cases and results #225'\n",
    "- [FAILED] Click on 'Build and share place where people can suggest their use cases and results #225'\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the issues page of the GitHub repository 'lavague-ai/LaVague'.\n",
    "- The objective is to go to the first issue.\n",
    "- Previous instructions have been unsuccessful. A new approach should be used.\n",
    "- The '#225' seems not to be clickable and it might be relevant to devise an instruction that does not include it. \n",
    "Next engine: Navigation Engine\n",
    "Instruction: Click on the first issue, with title 'Build and share place where people can suggest their use cases and results'\n",
    "-----\n",
    "Objective: Find When Llama 3 was released\n",
    "Previous instructions:\n",
    "- Click on 'meta-llama/Meta-Llama-3-8B'\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the model page for 'meta-llama/Meta-Llama-3-8B' on Hugging Face.\n",
    "- Hugging Face, is a hub for AI models and datasets, where users can explore and interact with a variety of AI models.\n",
    "- I am therefore on the right page to find information about the release date of 'Meta-Llama-3-8B'.\n",
    "- To find the release date, I need to locate the relevant information in the content of the page.\n",
    "- Therefore the best next step is to use the Python Engine to extract the release date from the content of the page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the release date of 'Meta-Llama-3-8B' from the textual content of the page.\n",
    "-----\n",
    "Objective: Provide the code to get started with Gemini API\n",
    "Previous instructions:\n",
    "- Click on 'Read API docs'\n",
    "- Click on 'Gemini API quickstart' on the menu\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the documentation page for the getting started of Gemini API.\n",
    "- I am therefore on the right page to find the code to get started with the Gemini API.\n",
    "- The next step is to provide the code to get started with the Gemini API.\n",
    "- Therefore I need to use the Python Engine to generate the code to extract the code to get started with the Gemini API from this page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the code to get started with the Gemini API from the content of the page.\n",
    "-----\n",
    "Objective: Show what is the cheapest product\n",
    "Previous instructions: [NONE]\n",
    "Last engine: [NONE]\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The screenshot shows an e-commerce website with various products.\n",
    "- To find the cheapest product, I need to identify the product with the lowest price.\n",
    "- There seems to be selectors for sorting products by price on the left side of the page.\n",
    "- The screenshot only shows part of the selectors for price. I should scroll down to see the full list of products and prices.\n",
    "Next engine: Navigation Controls\n",
    "Instruction: SCROLL_DOWN\n",
    "-----\n",
    "Objective: What tech stack do we use?\n",
    "Previous instructions: \n",
    "- [FAILED] Locate and click on the \"Technology Solutions\" link or section to find information about the tech stack.\n",
    "- [FAILED] Click on the \"Technology Solutions\" section to explore detailed information about the tech stack.\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thought:\n",
    "- The screenshot shows a Notion webpage with information about a company called ACME INC.\n",
    "- It has information about the company, their services, and departments.\n",
    "- Previous instructions tried to click on \"Technology Solutions\" without success. This probably means that \"Technology Solutions\" is not clickable or reachable.\n",
    "- Other strategies have to be pursued to reach the objective.\n",
    "- There seems to be information at the end of the screen about departments, with mention of a 'Software development' section that could be promising.\n",
    "- The best next step is to scroll down to gather more information.\n",
    "Next engine: Navigation Controls\n",
    "Instruction: SCROLL_DOWN\n",
    "\"\"\"\n",
    "\n",
    "WORLD_MODEL_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in high level reasoning. Your goal is to generate instructions for other specialized AIs to perform web actions to reach objectives given by humans.\n",
    "Your inputs are:\n",
    "- objective ('str'): a high level description of the goal to achieve.\n",
    "- previous_instructions ('str'): a list of previous steps taken to reach the objective.\n",
    "- last_engine ('str'): the engine used in the previous step.\n",
    "- current_relevant_state ('obj'): the state of the environment to use to perform the next step. This can be a screenshot if the previous engine was a NavigationEngine, or description of variables if the previous engine was a PythonEngine.\n",
    "\n",
    "Your output are:\n",
    "- thoughts ('str'): a list of thoughts in bullet points detailling your reasoning.\n",
    "- next_engine ('str'): the engine to use for the next step.\n",
    "- instruction ('str'): the instruction for the engine to perform the next step.\n",
    "\n",
    "Here are the engines at your disposal:\n",
    "- Python Engine: This engine is used when the task requires doing computing using the current state of the agent. \n",
    "It does not impact the outside world and does not navigate.\n",
    "- Navigation Engine: This engine is used when the next step of the task requires further navigation to reach the goal. \n",
    "For instance it can be used to click on a link or to fill a form on a webpage. This engine is heavy and will do complex processing of the current HTML to decide which element to interact with.\n",
    "- Navigation Controls: This is a simpler engine to do commands that does not require reasoning, which are 'SCROLL_DOWN', 'SCROLL_UP' and 'WAIT'.\n",
    "\n",
    "The instruction should be detailled as possible and only contain the next step. \n",
    "When providing an instruction to the Python Engine, do not provide any guideline on using visual information such as the screenshot, as the Python Engine does not have access to it.\n",
    "If the screenshot provides information but seems insufficient, use navigation controls to further explore the page.\n",
    "When providing information for the Navigation Engine, focus on elements that are most likely interactable, such as buttons, links, or forms and be precise in your description of the element to avoid ambiguitiy.\n",
    "If the objective is already achieved in the screenshot, or the state contains the demanded information, provide the next engine and instruction 'STOP'.\n",
    "If the objective requires information gathering, and the previous step was a Navigation step, do not directly stop when seeing the information but use the Python Engine to gather as much information as possible.\n",
    "If previous instructions failed, denoted by [FAILED], reflect on the mistake, and try to leverage other visual and textual cues to reach the objective.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next objective:\n",
    "Objective: {objective}\n",
    "Previous instructions: \n",
    "{previous_instructions}\n",
    "Last engine: {last_engine}\n",
    "Current relevant state: {current_state}\n",
    "\n",
    "Thought:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:18: UserWarning: \u001b[93mTelemetry is turned on. To turn off telemetry, set your TELEMETRY_VAR to 'NONE'\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n",
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:24: UserWarning: \u001b[93mSecurity warning: This package executes LLM-generated code. Consider using this package in a sandboxed environment.\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from lavague.contexts.openai import OpenaiContext\n",
    "\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
    "llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")\n",
    "\n",
    "context = OpenaiContext()\n",
    "context.llm = llm\n",
    "context.embedding = embed_model\n",
    "\n",
    "mm_llm = OpenAIMultiModal(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "    examples=world_model_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.groq import Groq\n",
    "# from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "# from llama_index.llms.gemini import Gemini\n",
    "\n",
    "# from lavague.contexts.openai import OpenaiContext\n",
    "\n",
    "# mm_llm = OpenAIMultiModal(model=\"gpt-4o\")\n",
    "# action_engine_llm = Groq(model=\"llama3-8b-8192\")\n",
    "# embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
    "# python_engine_llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")\n",
    "\n",
    "# context = OpenaiContext()\n",
    "# context.llm = action_engine_llm\n",
    "# context.embedding = embed_model\n",
    "\n",
    "# prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "#     examples=world_model_examples\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavague.core import Context, get_default_context\n",
    "from typing import Optional\n",
    "\n",
    "REWRITER_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI expert.\n",
    "You are given a high level instruction on a generic action to perform.\n",
    "Your output is an instruction of the action, rewritten to be more specific on the capabilities at your disposal to perform the action.\n",
    "Here are your capabilities:\n",
    "{capabilities}\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next instruction to rewrite:\n",
    "Original instruction: {original_instruction}\n",
    "\"\"\")\n",
    "\n",
    "DEFAULT_CAPABILITIES = \"\"\"\n",
    "- Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_EXAMPLES = \"\"\"\n",
    "Original instruction: Use the content of the HTML page to answer the question 'How was falcon-11B trained?'\n",
    "Capability: Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "Rewritten instruction: Extract the content of the HTML page and use llama index to answer the question 'How was falcon-11B trained?'\n",
    "\"\"\"\n",
    "\n",
    "class Rewriter:\n",
    "    def __init__(self, capabilities: str = DEFAULT_CAPABILITIES, examples: str = DEFAULT_EXAMPLES, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.prompt_template = REWRITER_PROMPT_TEMPLATE.partial_format(\n",
    "            capabilities=capabilities,\n",
    "            examples=examples\n",
    "        )      \n",
    "    def rewrite_instruction(self, original_instruction: str) -> str:\n",
    "        prompt = self.prompt_template.format(original_instruction=original_instruction)\n",
    "        rewritten_instruction = self.llm.complete(prompt).text\n",
    "        return rewritten_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import Context, get_default_context\n",
    "from llama_index.core.base.llms.base import BaseLLM\n",
    "import copy\n",
    "\n",
    "PYTHON_ENGINE_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in Python code generation to answer user queries.\n",
    "The inputs are: an instruction, and the current state of the local variables available to the environment where your code will be executed.\n",
    "Your output is the code that will perform the action described in the instruction, using the variables available in the environment.\n",
    "You can import libraries and use any variables available in the environment.\n",
    "Detail thoroughly the steps to perform the action in the code you generate with comments.\n",
    "The last line of your code should be an assignment to the variable 'output' containing the result of the action.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Instruction: {instruction}\n",
    "State:\n",
    "{state_description}\n",
    "Code:\n",
    "\"\"\")\n",
    "\n",
    "class PythonEngine:\n",
    "    llm: BaseLLM    \n",
    "    prompt_template: PromptTemplate\n",
    "    rewriter: Rewriter\n",
    "\n",
    "    def __init__(self, examples: str, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.extractor = context.extractor\n",
    "        self.prompt_template = PYTHON_ENGINE_PROMPT_TEMPLATE.partial_format(\n",
    "            examples=examples\n",
    "        )\n",
    "        self.rewriter = Rewriter(context=context)\n",
    "        \n",
    "    def generate_code(self, instruction: str, state: dict) -> str:\n",
    "        rewriter = self.rewriter\n",
    "        rewritten_instruction = rewriter.rewrite_instruction(instruction)\n",
    "        \n",
    "        state_description = self.get_state_description(state)\n",
    "        prompt = self.prompt_template.format(instruction=rewritten_instruction, state_description=state_description)\n",
    "        response = self.llm.complete(prompt).text\n",
    "        code = self.extractor.extract(response)\n",
    "        if not code:\n",
    "            raise ValueError(f\"No code generated or extracted. Here is the original response: {response}\")\n",
    "        return code\n",
    "    \n",
    "    def execute_code(self, code: str, state: dict):\n",
    "        local_scope = copy.deepcopy(state)\n",
    "        exec(code, local_scope, local_scope)\n",
    "        output = local_scope[\"output\"]\n",
    "        return output\n",
    "        \n",
    "    def get_state_description(self, state: dict) -> str:\n",
    "        \"\"\"TO DO: provide more complex state descriptions\"\"\"\n",
    "        state_description = \"\"\"\n",
    "html ('str'): The content of the HTML page being analyzed\"\"\"\n",
    "        return state_description\n",
    "    \n",
    "with open(\"python_engine_examples.txt\") as f:\n",
    "    python_engine_examples = f.read()\n",
    "    \n",
    "python_engine = PythonEngine(python_engine_examples, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flemme de finir\n",
    "\n",
    "# Thoughts:\n",
    "# - The current page is the Azure Pricing Calculator.\n",
    "# - To calculate the cost of an AKS (Azure Kubernetes Service) cluster, we need to add the AKS service to the calculator.\n",
    "# - The current screen does not show the options to add services.\n",
    "# - The best approach is to scroll down to find the section where services can be added to the calculator.\n",
    "# - Because this is a simple scrolling action, the best next step is to use the Navigation Controls engine to scroll down.\n",
    "# Next engine: Navigation Controls\n",
    "# Instruction: Scroll down by one full screen to continue exploring the current page.\n",
    "# -----\n",
    "# Objective: Find the definition of 'Diffusers'\n",
    "# Previous instructions: \n",
    "# - Click on 'Diffusers' link\n",
    "# Last engine: Navigation Engine\n",
    "# Current state: [SCREENSHOT]\n",
    "\n",
    "# Thought:\n",
    "# - The current page is the documentation page of Hugging Face.\n",
    "# - Hugging Face is a platform for AI models and datasets, where users can explore and interact with latest AI resources.\n",
    "# - The definition of 'Diffusers' is provided in the documentation.\n",
    "# - No further action is needed to achieve the objective.\n",
    "# Next engine: STOP\n",
    "# Instruction: STOP -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "width = 1024\n",
    "height = 1024\n",
    "\n",
    "driver.set_window_size(width, height)\n",
    "viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "\n",
    "height_difference = height - viewport_height\n",
    "driver.set_window_size(width, height + height_difference)\n",
    "\n",
    "driver.get(\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_instruction(text):\n",
    "    # Use a regular expression to find the content after \"Instruction:\"\n",
    "    instruction_patterns = [\n",
    "        r\"Instruction:\\s*(.*)\",\n",
    "        r\"### Instruction:\\s*(.*)\"\n",
    "    ]\n",
    "    for pattern in instruction_patterns:\n",
    "        instruction_match = re.search(pattern, text)\n",
    "        if instruction_match:\n",
    "            return instruction_match.group(\n",
    "                1\n",
    "            ).strip()  # Return the matched group, stripping any excess whitespace\n",
    "        \n",
    "    raise ValueError(\"No instruction found in the text.\")\n",
    "\n",
    "def extract_next_engine(text):\n",
    "    # Use a regular expression to find the content after \"Next engine:\"\n",
    "    \n",
    "    next_engine_patterns = [\n",
    "        r\"Next engine:\\s*(.*)\",\n",
    "        r\"### Next Engine:\\s*(.*)\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in next_engine_patterns:\n",
    "        next_engine_match = re.search(pattern, text)\n",
    "        if next_engine_match:\n",
    "            return next_engine_match.group(\n",
    "                1\n",
    "            ).strip()\n",
    "    raise ValueError(\"No next engine found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import ActionEngine\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "\n",
    "selenium_driver = SeleniumDriver()\n",
    "selenium_driver.driver = driver\n",
    "navigation_engine = ActionEngine(selenium_driver)\n",
    "action_engine = navigation_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# TO DO: Generalize to BaseDriver to support other drivers\n",
    "\n",
    "def scroll_down_one_viewport(driver):\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "\n",
    "    body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "    num_scrolls = viewport_height // 50  # Assuming each arrow key press scrolls 50 pixels\n",
    "    for _ in range(num_scrolls):\n",
    "        body.send_keys(Keys.ARROW_DOWN)\n",
    "        time.sleep(0.05)  \n",
    "        \n",
    "def scroll_up_one_viewport(driver):\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "\n",
    "    body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "    num_scrolls = viewport_height // 50  # Assuming each arrow key press scrolls 50 pixels\n",
    "    for _ in range(num_scrolls):\n",
    "        body.send_keys(Keys.ARROW_UP)\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "class NavigationControl:\n",
    "    def __init__(self, driver) -> None:\n",
    "        self.driver = driver\n",
    "        \n",
    "    def execute_instruction(self, instruction):\n",
    "        if 'SCROLL_DOWN' in instruction:\n",
    "            driver = self.driver\n",
    "            scroll_down_one_viewport(driver)\n",
    "        elif 'SCROLL_UP' in instruction:\n",
    "            driver = self.driver\n",
    "            scroll_up_one_viewport(driver)\n",
    "        elif 'WAIT' in instruction:\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown instruction: {instruction}\")\n",
    "        \n",
    "navigation_control = NavigationControl(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.multi_modal_llms import MultiModalLLM\n",
    "\n",
    "from lavague.core import Context, get_default_context\n",
    "\n",
    "class WorldModel:\n",
    "    \"\"\"Abstract class for WorldModel\"\"\"\n",
    "\n",
    "    mm_llm: MultiModalLLM\n",
    "    prompt_template: PromptTemplate\n",
    "\n",
    "    def __init__(self, examples: str, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.mm_llm = context.mm_llm\n",
    "        self.prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "            examples=examples\n",
    "        )\n",
    "        \n",
    "    def get_instruction(self, objective: str, previous_instructions, last_engine, current_state, image_documents) -> str:\n",
    "        \"\"\"Use GPT*V to generate instruction from the current state and objective.\"\"\"\n",
    "        mm_llm = self.mm_llm\n",
    "        \n",
    "        prompt = prompt_template.format(\n",
    "            objective=objective, previous_instructions=previous_instructions, \n",
    "            last_engine=last_engine, current_state=current_state)\n",
    "        \n",
    "        mm_llm_output = mm_llm.complete(prompt, image_documents=image_documents).text\n",
    "\n",
    "        return mm_llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavague.core.utilities.format_utils import extract_instruction\n",
    "from lavague.core import ActionEngine\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "N_ATTEMPTS = 5\n",
    "N_STEPS = 10\n",
    "\n",
    "class WebAgent:\n",
    "    \"\"\"\n",
    "    Web agent class, for now only works with selenium.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_engine: ActionEngine, python_engine: PythonEngine, world_model: WorldModel):\n",
    "        driver = action_engine.driver\n",
    "        \n",
    "        self.driver: SeleniumDriver = driver\n",
    "        self.action_engine: ActionEngine = action_engine\n",
    "        self.world_model: WorldModel = world_model\n",
    "        self.navigation_control = NavigationControl(driver)\n",
    "        self.python_engine = python_engine\n",
    "        \n",
    "        # Variable to store the output of the agent\n",
    "        self.output = None\n",
    "\n",
    "    def get(self, url):\n",
    "        self.driver.goto(url)\n",
    "        \n",
    "    def run(self, objective: str, display_in_notebook: bool = False):\n",
    "        world_model = self.world_model\n",
    "        action_engine = self.action_engine\n",
    "        driver: WebDriver = self.driver.driver\n",
    "        \n",
    "        previous_instructions = \"[NONE]\"\n",
    "        last_engine = \"[NONE]\"\n",
    "        \n",
    "        current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "\"\"\"\n",
    "\n",
    "        # TO DO: Don't save on disk the screenshot but do it in memory\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "        \n",
    "        for i in range(N_STEPS):\n",
    "            \n",
    "            mm_llm_output = world_model.get_instruction(\n",
    "                objective, previous_instructions, last_engine, \n",
    "                current_state, image_documents)\n",
    "            \n",
    "            print(\"LLM output: \", mm_llm_output)\n",
    "            \n",
    "            next_engine = extract_next_engine(mm_llm_output)\n",
    "            instruction = extract_instruction(mm_llm_output)\n",
    "            \n",
    "            if next_engine == \"Navigation Engine\":\n",
    "                \n",
    "                query = instruction\n",
    "                nodes = action_engine.get_nodes(query)\n",
    "                llm_context = \"\\n\".join(nodes)\n",
    "\n",
    "                for _ in range(N_ATTEMPTS):\n",
    "                    try:\n",
    "                        action = action_engine.get_action_from_context(llm_context, query)\n",
    "                        code = f\"\"\"\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "                        {action}\"\"\".strip()\n",
    "\n",
    "                        local_scope = {\"driver\": driver}\n",
    "                        exec(code, local_scope, local_scope)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Action execution failed. Retrying...\")\n",
    "                        pass\n",
    "                    \n",
    "                driver.save_screenshot(\"screenshots/output.png\")\n",
    "                image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "                current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "                \"\"\"\n",
    "                    \n",
    "            elif next_engine == \"Python Engine\":\n",
    "\n",
    "                state = {\n",
    "                    \"html\": driver.page_source\n",
    "                }\n",
    "\n",
    "                for _ in range(N_ATTEMPTS):\n",
    "                    try:\n",
    "                        code = python_engine.generate_code(instruction, state)\n",
    "                        output = python_engine.execute_code(code, state)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Python engine execution failed. Retrying...\")\n",
    "                        pass\n",
    "                print(\"Output generated by Python Engine: \", output)\n",
    "                print(\"Code generated by Python Engine: \", code)\n",
    "                \n",
    "                current_state = f\"\"\"\n",
    "- output: {output}\"\"\"\n",
    "                self.output = output\n",
    "            elif next_engine == \"Navigation Controls\":\n",
    "                navigation_control.execute_instruction(instruction)\n",
    "                driver.save_screenshot(\"screenshots/output.png\")\n",
    "                image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "                \n",
    "            elif next_engine == \"STOP\" or instruction == \"STOP\":\n",
    "                print(\"Objective reached. Stopping...\")\n",
    "                return self.output\n",
    "            \n",
    "            if previous_instructions == \"[NONE]\":\n",
    "                previous_instructions = f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "            else:\n",
    "                previous_instructions += f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "                \n",
    "            last_engine = next_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_model = WorldModel(world_model_examples, context)\n",
    "# agent = WebAgent(action_engine, python_engine, world_model)\n",
    "\n",
    "# url = \"https://huggingface.co/\"\n",
    "# objective = \"Provide code to run falcon 11b\"\n",
    "\n",
    "# agent.get(url)\n",
    "# output = agent.run(objective)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughts:\n",
      "- The screenshot shows the homepage of Hugging Face.\n",
      "- The objective is to print the code to use the hottest model on Hugging Face.\n",
      "- The \"Trending on\" section at the bottom of the screenshot lists models, spaces, and datasets.\n",
      "- The \"meta-llama/Meta-Llama-3-8B\" model is listed under the \"Trending on\" section, which suggests it might be the hottest model.\n",
      "- To confirm and get the code to use this model, I need to navigate to the model's page.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the \"meta-llama/Meta-Llama-3-8B\" model under the \"Trending on\" section.\n",
      "Thoughts:\n",
      "- The current screenshot shows the model page for \"meta-llama/Meta-Llama-3-8B\" on Hugging Face.\n",
      "- The objective is to print the code to use the hottest model on Hugging Face.\n",
      "- The page contains various sections including \"Model card\", \"Files and versions\", \"Inference API\", etc.\n",
      "- The \"Inference API\" section on the right side of the page likely contains the code to use the model.\n",
      "- The next step is to extract the code from the \"Inference API\" section.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Extract the code to use the \"meta-llama/Meta-Llama-3-8B\" model from the \"Inference API\" section of the page.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the \"Inference API\" section of the HTML page. \n",
      "# Then we will use Llama Index to identify and extract the code snippet that uses the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "# We extract the text content of the \"Inference API\" section of the HTML page\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "soup = BeautifulSoup(html, 'html.parser')\n",
      "inference_api_section = soup.find('h2', text='Inference API')\n",
      "if inference_api_section:\n",
      "    inference_api_text = inference_api_section.find_next_sibling('pre').text\n",
      "else:\n",
      "    inference_api_text = ''\n",
      "\n",
      "# Next we will use Llama Index to perform RAG on the extracted text content\n",
      "from llama_index.core import Document, VectorStoreIndex\n",
      "\n",
      "documents = [Document(text=inference_api_text)]\n",
      "\n",
      "# We then build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      "# We will use the query engine to answer the question\n",
      "instruction = \"Extract the code snippet that uses the 'meta-llama/Meta-Llama-3-8B' model.\"\n",
      "\n",
      "# We finally store the output in the variable 'output'\n",
      "output = query_engine.query(instruction)\n",
      "Python engine execution failed. Retrying...\n",
      "Error:  Cannot build index from nodes with no content. Please ensure all nodes have content.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the \"Inference API\" section of the HTML page. \n",
      "# Then we will use Llama Index to identify and extract the code snippet that uses the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "# We extract the text content of the \"Inference API\" section of the HTML page\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "soup = BeautifulSoup(html, 'html.parser')\n",
      "inference_api_section = soup.find('h2', text='Inference API')\n",
      "if inference_api_section:\n",
      "    inference_api_text = inference_api_section.find_next_sibling('pre').text\n",
      "else:\n",
      "    inference_api_text = ''\n",
      "\n",
      "# Next we will use Llama Index to perform RAG on the extracted text content\n",
      "from llama_index.core import Document, VectorStoreIndex\n",
      "\n",
      "documents = [Document(text=inference_api_text)]\n",
      "\n",
      "# We then build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      "# We will use the query engine to answer the question\n",
      "instruction = \"Extract the code snippet that uses the 'meta-llama/Meta-Llama-3-8B' model.\"\n",
      "\n",
      "# We finally store the output in the variable 'output'\n",
      "output = query_engine.query(instruction)\n",
      "Python engine execution failed. Retrying...\n",
      "Error:  Cannot build index from nodes with no content. Please ensure all nodes have content.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the \"Inference API\" section of the HTML page. \n",
      "# Then we will use Llama Index to search for the code snippet that uses the \"meta-llama/Meta-Llama-3-8B\" model within the extracted content.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "# We extract the text content of the \"Inference API\" section of the HTML page\n",
      "page_content = trafilatura.extract(html, include_section_titles=True, section_title=\"Inference API\")\n",
      "\n",
      "# Next we will use Llama Index to perform RAG on the extracted text content\n",
      "from llama_index.core import Document, VectorStoreIndex\n",
      "\n",
      "documents = [Document(text=page_content)]\n",
      "\n",
      "# We then build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      "# We will use the query engine to answer the question\n",
      "instruction = \"Find the code snippet that uses the 'meta-llama/Meta-Llama-3-8B' model\"\n",
      "\n",
      "# We finally store the output in the variable 'output'\n",
      "output = query_engine.query(instruction)\n",
      "Output generated by Python Engine:  >>> import transformers\n",
      ">>> import torch\n",
      ">>> model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
      ">>> pipeline = transformers.pipeline(\n",
      "\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
      ")\n",
      ">>> pipeline(\"Hey how are you doing today?\")\n",
      "Thoughts:\n",
      "- The current screenshot shows the page for the \"meta-llama/Meta-Llama-3-8B\" model on Hugging Face.\n",
      "- The Python Engine has already extracted the code to use the \"meta-llama/Meta-Llama-3-8B\" model from the \"Inference API\" section.\n",
      "- The objective is to print the code to use the hottest model on Hugging Face.\n",
      "- The code has been successfully extracted and is visible in the current state.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Print the extracted code to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the HTML page. \n",
      "# Then we will use regular expressions to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "page_content = trafilatura.extract(html)\n",
      "\n",
      "# Next we will use regular expressions to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "import re\n",
      "\n",
      "# We will use a regular expression to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "# The regular expression will look for a code block that contains the string \"meta-llama/Meta-Llama-3-8B\".\n",
      "# We will use the re.findall function to find all matches.\n",
      "code_snippet = re.findall(r'\n",
      "Python engine execution failed. Retrying...\n",
      "Error:  unterminated string literal (detected at line 16) (<string>, line 16)\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the HTML page. \n",
      "# Then we will use regular expressions to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "page_content = trafilatura.extract(html)\n",
      "\n",
      "# Next we will use regular expressions to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "import re\n",
      "\n",
      "# We will use a regular expression to find the code snippet that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "# The regular expression will look for the string \"meta-llama/Meta-Llama-3-8B\" and then capture the code snippet that follows.\n",
      "pattern = r\"meta-llama/Meta-Llama-3-8B(.*?)\n",
      "Python engine execution failed. Retrying...\n",
      "Error:  unterminated string literal (detected at line 15) (<string>, line 15)\n",
      "Code generated by Python Engine:  # We will use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "# We will use BeautifulSoup to parse the HTML content\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# We will use regular expressions to extract the code snippet\n",
      "import re\n",
      "\n",
      "# We first extract the text content of the HTML page\n",
      "page_content = trafilatura.extract(html)\n",
      "\n",
      "# We then parse the HTML content\n",
      "soup = BeautifulSoup(page_content, 'html.parser')\n",
      "\n",
      "# We then find all code snippets\n",
      "code_snippets = soup.find_all('code')\n",
      "\n",
      "# We then iterate over the code snippets and find the one that demonstrates how to use the \"meta-llama/Meta-Llama-3-8B\" model\n",
      "for code_snippet in code_snippets:\n",
      "    if \"meta-llama/Meta-Llama-3-8B\" in code_snippet.text:\n",
      "        # We then extract the code snippet\n",
      "        output = code_snippet.text\n",
      "        break\n",
      "Python engine execution failed. Retrying...\n",
      "Error:  'output'\n",
      "Thoughts:\n",
      "- The current screenshot shows the page for the \"meta-llama/Meta-Llama-3-8B\" model on Hugging Face.\n",
      "- The \"Inference API\" section is visible on the right side of the page.\n",
      "- The objective is to print the code to use the \"meta-llama/Meta-Llama-3-8B\" model.\n",
      "- The previous step to extract the code from the \"Inference API\" section failed, so I need to ensure the code is correctly identified and extracted.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Extract the code snippet from the \"Inference API\" section on the right side of the page and print it.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5300/3312503030.py\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ATTEMPTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mpython_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code generated by Python Engine: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5300/2138665676.py\u001b[0m in \u001b[0;36mgenerate_code\u001b[0;34m(self, instruction, state)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mstate_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewritten_instruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_description\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py\u001b[0m in \u001b[0;36mwrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 )\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mf_return_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_return_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;31m# intercept the generator and add a callback to the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/llama_index/llms/gemini/base.py\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     ) -> CompletionResponse:\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion_from_gemini_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, request_options)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_since_first_attempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m         \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_call_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             )\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0m_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "N_ATTEMPTS = 3\n",
    "N_STEPS = 13\n",
    "TIME_BETWEEN_ACTIONS = 1.5\n",
    "\n",
    "# url = \"https://maize-paddleboat-93e.notion.site/Welcome-to-ACME-INC-0ac66cd290e3453b93a993e1a3ed272f\"\n",
    "# objective = \"What's the name of our Lead Software Dev?\"\n",
    "\n",
    "# url = \"https://maize-paddleboat-93e.notion.site/Welcome-to-ACME-INC-0ac66cd290e3453b93a993e1a3ed272f\"\n",
    "# objective = \"Who is in the Software Quality Assurance team?\"\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# objective = \"Provide the code to use Falcon 11B\"\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# objective = \"Provide information Waifu diffusion\"\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# objective = \"Provide code to use DeepSeek-V2-Chat with transformers\"\n",
    "\n",
    "# url = \"https://huggingface.co/docs\"\n",
    "# objective = \"Provide the code to install PEFT\"\n",
    "\n",
    "url = \"https://huggingface.co/\"\n",
    "objective = \"Print the code to use the hotest model on Hugging Face\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "previous_instructions = \"[NONE]\"\n",
    "last_engine = \"[NONE]\"\n",
    "\n",
    "driver.save_screenshot(\"screenshots/output.png\")\n",
    "image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "\"\"\"\n",
    "\n",
    "for i in range(N_STEPS):\n",
    "    prompt = prompt_template.format(\n",
    "        objective=objective, previous_instructions=previous_instructions, \n",
    "        last_engine=last_engine, current_state=current_state)\n",
    "\n",
    "    mm_llm_output = mm_llm.complete(prompt, image_documents=image_documents).text\n",
    "\n",
    "    print(mm_llm_output)\n",
    "\n",
    "    next_engine = extract_next_engine(mm_llm_output)\n",
    "    instruction = extract_instruction(mm_llm_output)\n",
    "    \n",
    "    if \"Navigation Engine\" in next_engine:\n",
    "        \n",
    "        query = instruction\n",
    "        nodes = action_engine.get_nodes(query)\n",
    "        llm_context = \"\\n\".join(nodes)\n",
    "        \n",
    "        success = False\n",
    "\n",
    "        for _ in range(N_ATTEMPTS):\n",
    "            try:\n",
    "                action = action_engine.get_action_from_context(llm_context, query)\n",
    "                action_code = f\"\"\"\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "                {action}\"\"\".strip()\n",
    "\n",
    "                local_scope = {\"driver\": driver}\n",
    "                exec(action_code, local_scope, local_scope)\n",
    "                \n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Action execution failed. Retrying...\")\n",
    "                pass\n",
    "        if not success:\n",
    "            instruction = \"[FAILED] \" + instruction\n",
    "        time.sleep(TIME_BETWEEN_ACTIONS)\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "        current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\"\"\"\n",
    "            \n",
    "    elif \"Python Engine\" in next_engine:\n",
    "        state = {\n",
    "            \"html\": driver.page_source\n",
    "        }\n",
    "        success = False\n",
    "\n",
    "        for _ in range(N_ATTEMPTS):\n",
    "            try:\n",
    "                python_code = python_engine.generate_code(instruction, state)\n",
    "                print(\"Code generated by Python Engine: \", python_code)\n",
    "                output = python_engine.execute_code(python_code, state)\n",
    "                print(\"Output generated by Python Engine: \", output)\n",
    "                \n",
    "                current_state = f\"\"\"\n",
    "- output: {output}\"\"\"\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Python engine execution failed. Retrying...\")\n",
    "                print(\"Error: \", e)\n",
    "                current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\"\"\"\n",
    "                pass\n",
    "        \n",
    "        if not success:\n",
    "            instruction = \"[FAILED] \" + instruction\n",
    "        \n",
    "    elif \"Navigation Controls\" in next_engine:\n",
    "        navigation_control.execute_instruction(instruction)\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "        \n",
    "    elif next_engine == \"STOP\" or instruction == \"STOP\":\n",
    "        print(\"Objective reached. Stopping...\")\n",
    "        break\n",
    "    \n",
    "    if previous_instructions == \"[NONE]\":\n",
    "        previous_instructions = f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "    else:\n",
    "        previous_instructions += f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "        \n",
    "    last_engine = next_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Click on the \"meta-llama/Meta-Llama-3-8B\" model under the \"Trending on\" section.\n",
      "- Extract the code to use the \"meta-llama/Meta-Llama-3-8B\" model from the \"Inference API\" section of the page.\n",
      "- [FAILED] Print the extracted code to use the \"meta-llama/Meta-Llama-3-8B\" model.\n"
     ]
    }
   ],
   "source": [
    "print(previous_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the code snippet for using DeepSeek-V2-Chat with Hugging Face's Transformers:\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
      "\n",
      "model_name = \"deepseek-ai/DeepSeek-V2-Chat\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
      "\n",
      "max_memory = {i: \"75GB\" for i in range(8)}\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"sequential\", torch_dtype=torch.bfloat16, max_memory=max_memory, attn_implementation=\"eager\")\n",
      "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
      "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
      "\n",
      "messages = [\n",
      "    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n",
      "]\n",
      "\n",
      "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
      "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n",
      "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
      "\n",
      "print(result)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
