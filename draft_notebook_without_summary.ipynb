{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "world_model_examples = \"\"\"\n",
    "Objective:  Go to the first issue you can find\n",
    "Previous instructions: \n",
    "- Click on 'Issues' with the number '28' next to it.\n",
    "- [FAILED] Click on 'Build and share place where people can suggest their use cases and results #225'\n",
    "- [FAILED] Click on 'Build and share place where people can suggest their use cases and results #225'\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the issues page of the GitHub repository 'lavague-ai/LaVague'.\n",
    "- The objective is to go to the first issue.\n",
    "- Previous instructions have been unsuccessful. A new approach should be used.\n",
    "- The '#225' seems not to be clickable and it might be relevant to devise an instruction that does not include it. \n",
    "Next engine: Navigation Engine\n",
    "Instruction: Click on the first issue, with title 'Build and share place where people can suggest their use cases and results'\n",
    "-----\n",
    "Objective: Find When Llama 3 was released\n",
    "Previous instructions:\n",
    "- Click on 'meta-llama/Meta-Llama-3-8B'\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the model page for 'meta-llama/Meta-Llama-3-8B' on Hugging Face.\n",
    "- Hugging Face, is a hub for AI models and datasets, where users can explore and interact with a variety of AI models.\n",
    "- I am therefore on the right page to find information about the release date of 'Meta-Llama-3-8B'.\n",
    "- To find the release date, I need to locate the relevant information in the content of the page.\n",
    "- Therefore the best next step is to use the Python Engine to extract the release date from the content of the page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the release date of 'Meta-Llama-3-8B' from the textual content of the page.\n",
    "-----\n",
    "Objective: Provide the code to get started with Gemini API\n",
    "Previous instructions:\n",
    "- Click on 'Read API docs'\n",
    "- Click on 'Gemini API quickstart' on the menu\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The current screenshot shows the documentation page for the getting started of Gemini API.\n",
    "- I am therefore on the right page to find the code to get started with the Gemini API.\n",
    "- The next step is to provide the code to get started with the Gemini API.\n",
    "- Therefore I need to use the Python Engine to generate the code to extract the code to get started with the Gemini API from this page.\n",
    "Next engine: Python Engine\n",
    "Instruction: Extract the code to get started with the Gemini API from the content of the page.\n",
    "-----\n",
    "Objective: Show what is the cheapest product\n",
    "Previous instructions: [NONE]\n",
    "Last engine: [NONE]\n",
    "Current relevant state:\n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thoughts:\n",
    "- The screenshot shows an e-commerce website with various products.\n",
    "- To find the cheapest product, I need to identify the product with the lowest price.\n",
    "- There seems to be selectors for sorting products by price on the left side of the page.\n",
    "- The screenshot only shows part of the selectors for price. I should scroll down to see the full list of products and prices.\n",
    "Next engine: Navigation Controls\n",
    "Instruction: SCROLL_DOWN\n",
    "-----\n",
    "Objective: What tech stack do we use?\n",
    "Previous instructions: \n",
    "- [FAILED] Locate and click on the \"Technology Solutions\" link or section to find information about the tech stack.\n",
    "- [FAILED] Click on the \"Technology Solutions\" section to explore detailed information about the tech stack.\n",
    "Last engine: Navigation Engine\n",
    "Current relevant state: \n",
    "- screenshot: [SCREENSHOT]\n",
    "\n",
    "Thought:\n",
    "- The screenshot shows a Notion webpage with information about a company called ACME INC.\n",
    "- It has information about the company, their services, and departments.\n",
    "- Previous instructions tried to click on \"Technology Solutions\" without success. This probably means that \"Technology Solutions\" is not clickable or reachable.\n",
    "- Other strategies have to be pursued to reach the objective.\n",
    "- There seems to be information at the end of the screen about departments, with mention of a 'Software development' section that could be promising.\n",
    "- The best next step is to scroll down to gather more information.\n",
    "Next engine: Navigation Controls\n",
    "Instruction: SCROLL_DOWN\n",
    "\"\"\"\n",
    "\n",
    "WORLD_MODEL_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in high level reasoning. Your goal is to generate instructions for other specialized AIs to perform web actions to reach objectives given by humans.\n",
    "Your inputs are:\n",
    "- objective ('str'): a high level description of the goal to achieve.\n",
    "- previous_instructions ('str'): a list of previous steps taken to reach the objective.\n",
    "- last_engine ('str'): the engine used in the previous step.\n",
    "- current_relevant_state ('obj'): the state of the environment to use to perform the next step. This can be a screenshot if the previous engine was a NavigationEngine, or description of variables if the previous engine was a PythonEngine.\n",
    "\n",
    "Your output are:\n",
    "- thoughts ('str'): a list of thoughts in bullet points detailling your reasoning.\n",
    "- next_engine ('str'): the engine to use for the next step.\n",
    "- instruction ('str'): the instruction for the engine to perform the next step.\n",
    "\n",
    "Here are the engines at your disposal:\n",
    "- Python Engine: This engine is used when the task requires doing computing using the current state of the agent. \n",
    "It does not impact the outside world and does not navigate.\n",
    "- Navigation Engine: This engine is used when the next step of the task requires further navigation to reach the goal. \n",
    "For instance it can be used to click on a link or to fill a form on a webpage. This engine is heavy and will do complex processing of the current HTML to decide which element to interact with.\n",
    "- Navigation Controls: This is a simpler engine to do commands that does not require reasoning, which are 'SCROLL_DOWN', 'SCROLL_UP' and 'WAIT'.\n",
    "\n",
    "The instruction should be detailled as possible and only contain the next step. \n",
    "When providing an instruction to the Python Engine, do not provide any guideline on using visual information such as the screenshot, as the Python Engine does not have access to it.\n",
    "If the screenshot provides information but seems insufficient, use navigation controls to further explore the page.\n",
    "When providing information for the Navigation Engine, focus on elements that are most likely interactable, such as buttons, links, or forms and be precise in your description of the element to avoid ambiguitiy.\n",
    "If the objective is already achieved in the screenshot, or the state contains the demanded information, provide the next engine and instruction 'STOP'.\n",
    "If the objective requires information gathering, and the previous step was a Navigation step, do not directly stop when seeing the information but use the Python Engine to gather as much information as possible.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next objective:\n",
    "Objective: {objective}\n",
    "Previous instructions: \n",
    "{previous_instructions}\n",
    "Last engine: {last_engine}\n",
    "Current relevant state: {current_state}\n",
    "\n",
    "Thought:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:18: UserWarning: \u001b[93mTelemetry is turned on. To turn off telemetry, set your TELEMETRY_VAR to 'NONE'\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n",
      "/home/dhuynh/miniconda3/envs/myenv/lib/python3.10/site-packages/lavague/core/__init__.py:24: UserWarning: \u001b[93mSecurity warning: This package executes LLM-generated code. Consider using this package in a sandboxed environment.\u001b[0m\n",
      "  warnings.warn(warning_message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from lavague.contexts.openai import OpenaiContext\n",
    "\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
    "llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")\n",
    "\n",
    "context = OpenaiContext()\n",
    "context.llm = llm\n",
    "context.embedding = embed_model\n",
    "\n",
    "mm_llm = OpenAIMultiModal(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "    examples=world_model_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.groq import Groq\n",
    "# from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "# from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "# from llama_index.llms.gemini import Gemini\n",
    "\n",
    "# from lavague.contexts.openai import OpenaiContext\n",
    "\n",
    "# mm_llm = OpenAIMultiModal(model=\"gpt-4o\")\n",
    "# action_engine_llm = Groq(model=\"llama3-8b-8192\")\n",
    "# embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\")\n",
    "# python_engine_llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")\n",
    "\n",
    "# context = OpenaiContext()\n",
    "# context.llm = action_engine_llm\n",
    "# context.embedding = embed_model\n",
    "\n",
    "# prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "#     examples=world_model_examples\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavague.core import Context, get_default_context\n",
    "from typing import Optional\n",
    "\n",
    "REWRITER_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI expert.\n",
    "You are given a high level instruction on a generic action to perform.\n",
    "Your output is an instruction of the action, rewritten to be more specific on the capabilities at your disposal to perform the action.\n",
    "Here are your capabilities:\n",
    "{capabilities}\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Here is the next instruction to rewrite:\n",
    "Original instruction: {original_instruction}\n",
    "\"\"\")\n",
    "\n",
    "DEFAULT_CAPABILITIES = \"\"\"\n",
    "- Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_EXAMPLES = \"\"\"\n",
    "Original instruction: Use the content of the HTML page to answer the question 'How was falcon-11B trained?'\n",
    "Capability: Answer questions using the content of an HTML page using llama index and trafilatura\n",
    "Rewritten instruction: Extract the content of the HTML page and use llama index to answer the question 'How was falcon-11B trained?'\n",
    "\"\"\"\n",
    "\n",
    "class Rewriter:\n",
    "    def __init__(self, capabilities: str = DEFAULT_CAPABILITIES, examples: str = DEFAULT_EXAMPLES, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.prompt_template = REWRITER_PROMPT_TEMPLATE.partial_format(\n",
    "            capabilities=capabilities,\n",
    "            examples=examples\n",
    "        )      \n",
    "    def rewrite_instruction(self, original_instruction: str) -> str:\n",
    "        prompt = self.prompt_template.format(original_instruction=original_instruction)\n",
    "        rewritten_instruction = self.llm.complete(prompt).text\n",
    "        return rewritten_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import Context, get_default_context\n",
    "from llama_index.core.base.llms.base import BaseLLM\n",
    "import copy\n",
    "\n",
    "PYTHON_ENGINE_PROMPT_TEMPLATE = PromptTemplate(\"\"\"\n",
    "You are an AI system specialized in Python code generation to answer user queries.\n",
    "The inputs are: an instruction, and the current state of the local variables available to the environment where your code will be executed.\n",
    "Your output is the code that will perform the action described in the instruction, using the variables available in the environment.\n",
    "You can import libraries and use any variables available in the environment.\n",
    "Detail thoroughly the steps to perform the action in the code you generate with comments.\n",
    "The last line of your code should be an assignment to the variable 'output' containing the result of the action.\n",
    "\n",
    "Here are previous examples:\n",
    "{examples}\n",
    "\n",
    "Instruction: {instruction}\n",
    "State:\n",
    "{state_description}\n",
    "Code:\n",
    "\"\"\")\n",
    "\n",
    "class PythonEngine:\n",
    "    llm: BaseLLM    \n",
    "    prompt_template: PromptTemplate\n",
    "    rewriter: Rewriter\n",
    "\n",
    "    def __init__(self, examples: str, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.llm = context.llm\n",
    "        self.extractor = context.extractor\n",
    "        self.prompt_template = PYTHON_ENGINE_PROMPT_TEMPLATE.partial_format(\n",
    "            examples=examples\n",
    "        )\n",
    "        self.rewriter = Rewriter(context=context)\n",
    "        \n",
    "    def generate_code(self, instruction: str, state: dict) -> str:\n",
    "        rewriter = self.rewriter\n",
    "        rewritten_instruction = rewriter.rewrite_instruction(instruction)\n",
    "        \n",
    "        state_description = self.get_state_description(state)\n",
    "        prompt = self.prompt_template.format(instruction=rewritten_instruction, state_description=state_description)\n",
    "        response = self.llm.complete(prompt).text\n",
    "        code = self.extractor.extract(response)\n",
    "        if not code:\n",
    "            raise ValueError(f\"No code generated or extracted. Here is the original response: {response}\")\n",
    "        return code\n",
    "    \n",
    "    def execute_code(self, code: str, state: dict):\n",
    "        local_scope = copy.deepcopy(state)\n",
    "        exec(code, local_scope, local_scope)\n",
    "        output = local_scope[\"output\"]\n",
    "        return output\n",
    "        \n",
    "    def get_state_description(self, state: dict) -> str:\n",
    "        \"\"\"TO DO: provide more complex state descriptions\"\"\"\n",
    "        state_description = \"\"\"\n",
    "html ('str'): The content of the HTML page being analyzed\"\"\"\n",
    "        return state_description\n",
    "    \n",
    "with open(\"python_engine_examples.txt\") as f:\n",
    "    python_engine_examples = f.read()\n",
    "    \n",
    "python_engine = PythonEngine(python_engine_examples, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flemme de finir\n",
    "\n",
    "# Thoughts:\n",
    "# - The current page is the Azure Pricing Calculator.\n",
    "# - To calculate the cost of an AKS (Azure Kubernetes Service) cluster, we need to add the AKS service to the calculator.\n",
    "# - The current screen does not show the options to add services.\n",
    "# - The best approach is to scroll down to find the section where services can be added to the calculator.\n",
    "# - Because this is a simple scrolling action, the best next step is to use the Navigation Controls engine to scroll down.\n",
    "# Next engine: Navigation Controls\n",
    "# Instruction: Scroll down by one full screen to continue exploring the current page.\n",
    "# -----\n",
    "# Objective: Find the definition of 'Diffusers'\n",
    "# Previous instructions: \n",
    "# - Click on 'Diffusers' link\n",
    "# Last engine: Navigation Engine\n",
    "# Current state: [SCREENSHOT]\n",
    "\n",
    "# Thought:\n",
    "# - The current page is the documentation page of Hugging Face.\n",
    "# - Hugging Face is a platform for AI models and datasets, where users can explore and interact with latest AI resources.\n",
    "# - The definition of 'Diffusers' is provided in the documentation.\n",
    "# - No further action is needed to achieve the objective.\n",
    "# Next engine: STOP\n",
    "# Instruction: STOP -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "width = 1024\n",
    "height = 1024\n",
    "\n",
    "driver.set_window_size(width, height)\n",
    "viewport_height = driver.execute_script(\"return window.innerHeight;\")\n",
    "\n",
    "height_difference = height - viewport_height\n",
    "driver.set_window_size(width, height + height_difference)\n",
    "\n",
    "driver.get(\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_instruction(text):\n",
    "    # Use a regular expression to find the content after \"Instruction:\"\n",
    "    instruction_patterns = [\n",
    "        r\"Instruction:\\s*(.*)\",\n",
    "        r\"### Instruction:\\s*(.*)\"\n",
    "    ]\n",
    "    for pattern in instruction_patterns:\n",
    "        instruction_match = re.search(pattern, text)\n",
    "        if instruction_match:\n",
    "            return instruction_match.group(\n",
    "                1\n",
    "            ).strip()  # Return the matched group, stripping any excess whitespace\n",
    "        \n",
    "    raise ValueError(\"No instruction found in the text.\")\n",
    "\n",
    "def extract_next_engine(text):\n",
    "    # Use a regular expression to find the content after \"Next engine:\"\n",
    "    \n",
    "    next_engine_patterns = [\n",
    "        r\"Next engine:\\s*(.*)\",\n",
    "        r\"### Next Engine:\\s*(.*)\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in next_engine_patterns:\n",
    "        next_engine_match = re.search(pattern, text)\n",
    "        if next_engine_match:\n",
    "            return next_engine_match.group(\n",
    "                1\n",
    "            ).strip()\n",
    "    raise ValueError(\"No next engine found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavague.core import ActionEngine\n",
    "from lavague.drivers.selenium import SeleniumDriver\n",
    "\n",
    "selenium_driver = SeleniumDriver()\n",
    "selenium_driver.driver = driver\n",
    "navigation_engine = ActionEngine(selenium_driver)\n",
    "action_engine = navigation_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# TO DO: Generalize to BaseDriver to support other drivers\n",
    "\n",
    "def scroll_down_one_viewport(driver):\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "\n",
    "    body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "    num_scrolls = viewport_height // 50  # Assuming each arrow key press scrolls 50 pixels\n",
    "    for _ in range(num_scrolls):\n",
    "        body.send_keys(Keys.ARROW_DOWN)\n",
    "        time.sleep(0.05)  \n",
    "        \n",
    "def scroll_up_one_viewport(driver):\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "\n",
    "    body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "    num_scrolls = viewport_height // 50  # Assuming each arrow key press scrolls 50 pixels\n",
    "    for _ in range(num_scrolls):\n",
    "        body.send_keys(Keys.ARROW_UP)\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "class NavigationControl:\n",
    "    def __init__(self, driver) -> None:\n",
    "        self.driver = driver\n",
    "        \n",
    "    def execute_instruction(self, instruction):\n",
    "        if 'SCROLL_DOWN' in instruction:\n",
    "            driver = self.driver\n",
    "            scroll_down_one_viewport(driver)\n",
    "        elif 'SCROLL_UP' in instruction:\n",
    "            driver = self.driver\n",
    "            scroll_up_one_viewport(driver)\n",
    "        elif 'WAIT' in instruction:\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown instruction: {instruction}\")\n",
    "        \n",
    "navigation_control = NavigationControl(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader\n",
    "from llama_index.core.multi_modal_llms import MultiModalLLM\n",
    "\n",
    "from lavague.core import Context, get_default_context\n",
    "\n",
    "class WorldModel:\n",
    "    \"\"\"Abstract class for WorldModel\"\"\"\n",
    "\n",
    "    mm_llm: MultiModalLLM\n",
    "    prompt_template: PromptTemplate\n",
    "\n",
    "    def __init__(self, examples: str, context: Optional[Context] = None):\n",
    "        if context is None:\n",
    "            context = get_default_context()\n",
    "        self.mm_llm = context.mm_llm\n",
    "        self.prompt_template = WORLD_MODEL_PROMPT_TEMPLATE.partial_format(\n",
    "            examples=examples\n",
    "        )\n",
    "        \n",
    "    def get_instruction(self, objective: str, previous_instructions, last_engine, current_state, image_documents) -> str:\n",
    "        \"\"\"Use GPT*V to generate instruction from the current state and objective.\"\"\"\n",
    "        mm_llm = self.mm_llm\n",
    "        \n",
    "        prompt = prompt_template.format(\n",
    "            objective=objective, previous_instructions=previous_instructions, \n",
    "            last_engine=last_engine, current_state=current_state)\n",
    "        \n",
    "        mm_llm_output = mm_llm.complete(prompt, image_documents=image_documents).text\n",
    "\n",
    "        return mm_llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lavague.core.utilities.format_utils import extract_instruction\n",
    "from lavague.core import ActionEngine\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "N_ATTEMPTS = 5\n",
    "N_STEPS = 10\n",
    "\n",
    "class WebAgent:\n",
    "    \"\"\"\n",
    "    Web agent class, for now only works with selenium.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_engine: ActionEngine, python_engine: PythonEngine, world_model: WorldModel):\n",
    "        driver = action_engine.driver\n",
    "        \n",
    "        self.driver: SeleniumDriver = driver\n",
    "        self.action_engine: ActionEngine = action_engine\n",
    "        self.world_model: WorldModel = world_model\n",
    "        self.navigation_control = NavigationControl(driver)\n",
    "        self.python_engine = python_engine\n",
    "        \n",
    "        # Variable to store the output of the agent\n",
    "        self.output = None\n",
    "\n",
    "    def get(self, url):\n",
    "        self.driver.goto(url)\n",
    "        \n",
    "    def run(self, objective: str, display_in_notebook: bool = False):\n",
    "        world_model = self.world_model\n",
    "        action_engine = self.action_engine\n",
    "        driver: WebDriver = self.driver.driver\n",
    "        \n",
    "        previous_instructions = \"[NONE]\"\n",
    "        last_engine = \"[NONE]\"\n",
    "        \n",
    "        current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "\"\"\"\n",
    "\n",
    "        # TO DO: Don't save on disk the screenshot but do it in memory\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "        \n",
    "        for i in range(N_STEPS):\n",
    "            \n",
    "            mm_llm_output = world_model.get_instruction(\n",
    "                objective, previous_instructions, last_engine, \n",
    "                current_state, image_documents)\n",
    "            \n",
    "            print(\"LLM output: \", mm_llm_output)\n",
    "            \n",
    "            next_engine = extract_next_engine(mm_llm_output)\n",
    "            instruction = extract_instruction(mm_llm_output)\n",
    "            \n",
    "            if next_engine == \"Navigation Engine\":\n",
    "                \n",
    "                query = instruction\n",
    "                nodes = action_engine.get_nodes(query)\n",
    "                llm_context = \"\\n\".join(nodes)\n",
    "\n",
    "                for _ in range(N_ATTEMPTS):\n",
    "                    try:\n",
    "                        action = action_engine.get_action_from_context(llm_context, query)\n",
    "                        code = f\"\"\"\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "                        {action}\"\"\".strip()\n",
    "\n",
    "                        local_scope = {\"driver\": driver}\n",
    "                        exec(code, local_scope, local_scope)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Action execution failed. Retrying...\")\n",
    "                        pass\n",
    "                    \n",
    "                driver.save_screenshot(\"screenshots/output.png\")\n",
    "                image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "                current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "                \"\"\"\n",
    "                    \n",
    "            elif next_engine == \"Python Engine\":\n",
    "\n",
    "                state = {\n",
    "                    \"html\": driver.page_source\n",
    "                }\n",
    "\n",
    "                for _ in range(N_ATTEMPTS):\n",
    "                    try:\n",
    "                        code = python_engine.generate_code(instruction, state)\n",
    "                        output = python_engine.execute_code(code, state)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Python engine execution failed. Retrying...\")\n",
    "                        pass\n",
    "                print(\"Output generated by Python Engine: \", output)\n",
    "                print(\"Code generated by Python Engine: \", code)\n",
    "                \n",
    "                current_state = f\"\"\"\n",
    "- output: {output}\"\"\"\n",
    "                self.output = output\n",
    "            elif next_engine == \"Navigation Controls\":\n",
    "                navigation_control.execute_instruction(instruction)\n",
    "                driver.save_screenshot(\"screenshots/output.png\")\n",
    "                image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "                \n",
    "            elif next_engine == \"STOP\" or instruction == \"STOP\":\n",
    "                print(\"Objective reached. Stopping...\")\n",
    "                return self.output\n",
    "            \n",
    "            if previous_instructions == \"[NONE]\":\n",
    "                previous_instructions = f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "            else:\n",
    "                previous_instructions += f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "                \n",
    "            last_engine = next_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_model = WorldModel(world_model_examples, context)\n",
    "# agent = WebAgent(action_engine, python_engine, world_model)\n",
    "\n",
    "# url = \"https://huggingface.co/\"\n",
    "# objective = \"Provide code to run falcon 11b\"\n",
    "\n",
    "# agent.get(url)\n",
    "# output = agent.run(objective)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoughts:\n",
      "- The screenshot shows the homepage of Hugging Face, a platform for AI models and datasets.\n",
      "- To find the code to use DeepSeek-V2-Chat with transformers, I need to navigate to the specific model page or documentation.\n",
      "- The search functionality on the Hugging Face homepage can be used to locate the DeepSeek-V2-Chat model.\n",
      "- The next step is to use the search bar to find the DeepSeek-V2-Chat model.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the search bar and type \"DeepSeek-V2-Chat\" to search for the model.\n",
      "- The current screenshot shows the search results for \"DeepSeek-V2-Chat\" on Hugging Face.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The first search result is \"DeepSeek-ai/DeepSeek-V2-Chat\", which is likely the model we need.\n",
      "- The next step is to navigate to the \"DeepSeek-ai/DeepSeek-V2-Chat\" model page to find the relevant code.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on \"DeepSeek-ai/DeepSeek-V2-Chat\" from the search results.\n",
      "- The current screenshot shows the model page for \"DeepSeek-ai/DeepSeek-V2-Chat\" on Hugging Face.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant information or code snippet is likely to be found in the model card or documentation sections.\n",
      "- There are tabs and sections such as \"Model card\", \"Evaluation Results\", \"Model API\", etc., which might contain the required code.\n",
      "- The best next step is to explore the \"Model card\" section to find the code.\n",
      "\n",
      "Next engine: Navigation Engine\n",
      "Instruction: Click on the \"Model card\" tab to explore the details and find the code to use DeepSeek-V2-Chat with transformers.\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Action execution failed. Retrying...\n",
      "Thoughts:\n",
      "- The current screenshot shows the model page for \"DeepSeek-ai/DeepSeek-V2-Chat\" on Hugging Face.\n",
      "- The objective is to provide the code to use DeepSeek-V2-Chat with transformers.\n",
      "- The \"Model card\" tab is already selected, indicating that we are on the right page to find the necessary code.\n",
      "- The relevant information might be present on this page, but it is not fully visible in the current screenshot.\n",
      "- Scrolling down might reveal the code or additional sections that contain the code.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the \"Model card\" tab for \"DeepSeek-V2-Chat\".\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant information is likely to be found in the model card, which might include example code or links to documentation.\n",
      "- The current view does not show any code snippets or examples.\n",
      "- Scrolling down further might reveal the code or relevant sections.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the model card for \"DeepSeek-V2-Chat\" on Hugging Face.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant information might be further down the page, as the current view does not show any code snippets or examples.\n",
      "- Scrolling down further might reveal the code or instructions needed.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the \"Evaluation Results\" section of the DeepSeek-V2-Chat model page.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant code is likely to be found in the \"Model card\" or a similar section that provides usage instructions.\n",
      "- Since the previous steps involved scrolling down, it is possible that the relevant section is further down the page.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the details of the DeepSeek-V2-Chat model, including performance benchmarks.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant code is likely to be found in the model card or documentation section.\n",
      "- Since the previous instructions included multiple scrolls, it is possible that the relevant code is further down the page.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows evaluation results and some text about the performance of DeepSeek-V2-Chat.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant code is likely to be found in the model card or documentation section.\n",
      "- The previous instructions included multiple scrolls, indicating that the relevant information might be further down the page.\n",
      "- I should continue scrolling to explore more of the page to find the code.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the model card for \"DeepSeek-V2-Chat\" with some evaluation metrics and a section titled \"Coding Benchmarks.\"\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The \"Coding Benchmarks\" section might contain relevant information or code snippets.\n",
      "- The next step should be to further explore the \"Coding Benchmarks\" section to find the required code.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows the \"Model Architecture\" section of the DeepSeek-V2-Chat model page.\n",
      "- The objective is to provide code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant code is likely to be found in a section dedicated to usage or examples, which might be further down the page.\n",
      "- Since the current view does not show the required code, I should continue scrolling down to find the relevant section.\n",
      "\n",
      "Next engine: Navigation Controls\n",
      "Instruction: SCROLL_DOWN\n",
      "Thoughts:\n",
      "- The current screenshot shows a section of the documentation for DeepSeek-V2-Chat.\n",
      "- The content includes code snippets for using the model with transformers.\n",
      "- The objective is to provide the code to use DeepSeek-V2-Chat with transformers.\n",
      "- The relevant code snippets are visible in the screenshot.\n",
      "\n",
      "Next engine: Python Engine\n",
      "Instruction: Extract the code snippets for using DeepSeek-V2-Chat with transformers from the content of the page.\n",
      "Code generated by Python Engine:  # Let's think step by step\n",
      "# We first need to extract the text content of the HTML page. \n",
      "# Then we will use an LLM to answer the question. Because the page content might not fit in the LLM context window, we will use Llama Index to perform RAG on the extracted text content.\n",
      "\n",
      "# First, We use the trafilatura library to extract the text content of the HTML page\n",
      "import trafilatura\n",
      "\n",
      "page_content = trafilatura.extract(html)\n",
      "\n",
      "# Next we will use Llama Index to perform RAG on the extracted text content\n",
      "from llama_index.core import Document, VectorStoreIndex\n",
      "\n",
      "documents = [Document(text=page_content)]\n",
      "\n",
      "# We then build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "query_engine = index.as_query_engine()\n",
      "\n",
      "# We will use the query engine to answer the question\n",
      "instruction = \"Find and extract code snippets that demonstrate how to use DeepSeek-V2-Chat with transformers.\"\n",
      "\n",
      "# We finally store the output in the variable 'output'\n",
      "output = query_engine.query(instruction)\n",
      "Output generated by Python Engine:  To use DeepSeek-V2-Chat with transformers, you can refer to the following code snippet:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
      "\n",
      "model_name = \"deepseek-ai/DeepSeek-V2-Chat\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
      "\n",
      "max_memory = {i: \"75GB\" for i in range(8)}\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"sequential\", torch_dtype=torch.bfloat16, max_memory=max_memory, attn_implementation=\"eager\")\n",
      "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
      "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
      "\n",
      "messages = [\n",
      "    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n",
      "]\n",
      "\n",
      "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
      "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n",
      "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
      "\n",
      "print(result)\n",
      "```\n",
      "Thoughts:\n",
      "- The Python Engine has successfully extracted the code snippet for using DeepSeek-V2-Chat with transformers.\n",
      "- The objective is to provide the code to use DeepSeek-V2-Chat with transformers.\n",
      "- The extracted code snippet is visible in the current state.\n",
      "- The objective has been achieved.\n",
      "\n",
      "Next engine: STOP\n",
      "Instruction: STOP\n",
      "Objective reached. Stopping...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "N_ATTEMPTS = 3\n",
    "N_STEPS = 13\n",
    "TIME_BETWEEN_ACTIONS = 1.5\n",
    "\n",
    "# url = \"https://maize-paddleboat-93e.notion.site/Welcome-to-ACME-INC-0ac66cd290e3453b93a993e1a3ed272f\"\n",
    "# objective = \"What's the name of our Lead Software Dev?\"\n",
    "\n",
    "# url = \"https://maize-paddleboat-93e.notion.site/Welcome-to-ACME-INC-0ac66cd290e3453b93a993e1a3ed272f\"\n",
    "# objective = \"Who is in the Software Quality Assurance team?\"\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# objective = \"Provide the code to use Falcon 11B\"\n",
    "\n",
    "# url = \"https://huggingface.co\"\n",
    "# objective = \"Provide information Waifu diffusion\"\n",
    "\n",
    "url = \"https://huggingface.co\"\n",
    "objective = \"Provide code to use DeepSeek-V2-Chat with transformers\"\n",
    "\n",
    "# url = \"https://huggingface.co/docs\"\n",
    "# objective = \"Provide the code to install PEFT\"\n",
    "\n",
    "# url = \"https://huggingface.co/\"\n",
    "# objective = \"Print the code to use the hotest model on Hugging Face\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "previous_instructions = \"[NONE]\"\n",
    "last_engine = \"[NONE]\"\n",
    "\n",
    "driver.save_screenshot(\"screenshots/output.png\")\n",
    "image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\n",
    "\"\"\"\n",
    "\n",
    "for i in range(N_STEPS):\n",
    "    prompt = prompt_template.format(\n",
    "        objective=objective, previous_instructions=previous_instructions, \n",
    "        last_engine=last_engine, current_state=current_state)\n",
    "\n",
    "    mm_llm_output = mm_llm.complete(prompt, image_documents=image_documents).text\n",
    "\n",
    "    print(mm_llm_output)\n",
    "\n",
    "    next_engine = extract_next_engine(mm_llm_output)\n",
    "    instruction = extract_instruction(mm_llm_output)\n",
    "    \n",
    "    if \"Navigation Engine\" in next_engine:\n",
    "        \n",
    "        query = instruction\n",
    "        nodes = action_engine.get_nodes(query)\n",
    "        llm_context = \"\\n\".join(nodes)\n",
    "        \n",
    "        success = False\n",
    "\n",
    "        for _ in range(N_ATTEMPTS):\n",
    "            try:\n",
    "                action = action_engine.get_action_from_context(llm_context, query)\n",
    "                action_code = f\"\"\"\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "                {action}\"\"\".strip()\n",
    "\n",
    "                local_scope = {\"driver\": driver}\n",
    "                exec(action_code, local_scope, local_scope)\n",
    "                \n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Action execution failed. Retrying...\")\n",
    "                pass\n",
    "        if not success:\n",
    "            instruction = \"[FAILED] \" + instruction\n",
    "        time.sleep(TIME_BETWEEN_ACTIONS)\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "\n",
    "        current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\"\"\"\n",
    "            \n",
    "    elif \"Python Engine\" in next_engine:\n",
    "        state = {\n",
    "            \"html\": driver.page_source\n",
    "        }\n",
    "        success = False\n",
    "\n",
    "        for _ in range(N_ATTEMPTS):\n",
    "            try:\n",
    "                python_code = python_engine.generate_code(instruction, state)\n",
    "                print(\"Code generated by Python Engine: \", python_code)\n",
    "                output = python_engine.execute_code(python_code, state)\n",
    "                print(\"Output generated by Python Engine: \", output)\n",
    "                \n",
    "                current_state = f\"\"\"\n",
    "- output: {output}\"\"\"\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Python engine execution failed. Retrying...\")\n",
    "                print(\"Error: \", e)\n",
    "                current_state = f\"\"\"\n",
    "- screenshot: [SCREENSHOT]\"\"\"\n",
    "                pass\n",
    "        \n",
    "        if not success:\n",
    "            instruction = \"[FAILED] \" + instruction\n",
    "        \n",
    "    elif \"Navigation Controls\" in next_engine:\n",
    "        navigation_control.execute_instruction(instruction)\n",
    "        driver.save_screenshot(\"screenshots/output.png\")\n",
    "        image_documents = SimpleDirectoryReader(\"./screenshots\").load_data()\n",
    "        \n",
    "    elif next_engine == \"STOP\" or instruction == \"STOP\":\n",
    "        print(\"Objective reached. Stopping...\")\n",
    "        break\n",
    "    \n",
    "    if previous_instructions == \"[NONE]\":\n",
    "        previous_instructions = f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "    else:\n",
    "        previous_instructions += f\"\"\"\n",
    "- {instruction}\"\"\"\n",
    "        \n",
    "    last_engine = next_engine\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
